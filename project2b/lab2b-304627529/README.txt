Bradley Zhu - 304627529
Project 2A

SortedList.h is a header file describing the interfaces for linked list
operations

SortedList.c is a C module tha impliments methods for a sorted doubly linked
list.

lab2_list.c is a C program that impliments specified options and produces
statisitics.

My Makefile can build (compiles all programs), tests (run test cases to
generate CSV files), profile (tests with profiling tools and generates an execution
profileing report) graphs (uses gnuplot to create graphs), tarball (creates deliverable 
tarbell), and clean (cleans output).

pic12.csv is the outputs to create lab2b_1.png and lab2b_2.png.
lab_2b_list.csv is the outputs to create all the other pictures.

My various pictures are graphs generated by gnuplot.

My README.txt contains descriptions of files and answers to questions.

profile.gperf is my profiling report.

Question 2.3.1
	In 1 and 2-thread tests, for both add and list the majority of the time is
	probably spent doing the operations and overhead.  There would be none or little 
	contention so the threads wouldn't spend much time waiting for locks.
	In the high-thread spin-lock tests, most of the time is probably spent waiting
	for the other threads by spinning.  When a thread has the CPU time but it is
	locked, then nothing will really happen.
	In the high-thread mutex tests, most of the time is probably spent sleeping.
	The mutex has also has overhead when going to sleep and when waking up.  Most
	of the cycles are spent running the program though since sleeping threads
	don't actually run.

Question 2.3.2
	The line that takes the most cycles when the spin-lock version of the list
	exerciser is run with a large number of threads is:
	while(__sync_lock_test_and_set(&spin, 1));
	This operation becomes expensive with large numbers of threads because the
	number of collisions increases a lot.  With a lot of threads only one thread
	will be working while the rest of the threads will just be on that line
	spinning waiting for the lock to be released.

Question 2.3.3
	The average lock wait time rises so dramatically with the number of contending
	threads because with more threads, there is more contention.  The threads have
	to wait for other threads more, leading to more time spent waiting.  We also
	have each thread having to wait for more time as well as more threads waiting.
	The completion time per operation rises less dramatically because the
	completion time per operation is divided by the number of threads which is
	less than average lock wait time.  It rises because the more threads that we
	have, the more threads go to sleep.
	Wait time per operation goes up faster than completion time because we have
	O(N^2) for wait time, and O(N) completion time.

Question 2.3.4
	When there are fewer lists, there are more conflicts and more contention as
	more threads compete for the same resources.  The locks will be held longer
	and more threads will be waiting, resulting in longer times.  When there are
	more lists, there is less contention, so overall performance will go up.
	The throughput doesn't continue increasing because at a certain point you 
	will have more lists than elements and performance will stop increasing.
	According to my graphs generated, the throughput of an N-way partitioned list
	should be equivalent to the throughput of a single list with fewer (1/N)
	threads.
